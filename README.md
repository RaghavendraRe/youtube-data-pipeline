# GitHub Actions YouTube Data Pipeline

This directory contains an automated YouTube data collection pipeline using GitHub Actions.

## üìÅ Project Structure

```
data collection/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ youtube-etl.yml          # GitHub Actions workflow
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ brahmakumaris_videos.csv     # Output data (auto-updated)
‚îú‚îÄ‚îÄ fetch_youtube_data.py             # Python ETL script
‚îú‚îÄ‚îÄ requirements.txt                  # Python dependencies
‚îú‚îÄ‚îÄ config.yaml                       # Configuration file
‚îî‚îÄ‚îÄ README.md                         # This file
```

## ÔøΩ How it Works

```mermaid
graph TD
    A[üïí Scheduled Daily (00:00 UTC)] -->|Trigger| B(GitHub Actions Runner)
    B -->|Setup| C{Python Environment}
    C -->|Install| D[Dependencies]
    D -->|Run Script| E[fetch_youtube_data.py]
    
    E -->|Request Data| F((YouTube Data API))
    F -->|Return JSON| E
    
    E -->|Process & Transform| G[/brahmakumaris_videos.csv/]
    G -->|Commit & Push| H[GitHub Repository]
    
    H -->|Raw URL Connection| I[Power BI Dashboard]
    I -->|Scheduled Refresh| J[Auto-Updated Reports]

    style A fill:#f9f,stroke:#333
    style F fill:#ff0000,stroke:#333,color:white
    style I fill:#f2c811,stroke:#333
```

## ÔøΩüöÄ Quick Start

### 1Ô∏è‚É£ Initialize GitHub Repository

```bash
# Initialize git (if not already done)
git init

# Add all files
git add .

# Commit
git commit -m "Initial commit: YouTube ETL pipeline"

# Create GitHub repo and push
# Option A: Using GitHub CLI
gh repo create my-youtube-pipeline --public --source=. --remote=origin --push

# Option B: Manual (create repo on GitHub.com first, then)
git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git
git branch -M main
git push -u origin main
```

### 2Ô∏è‚É£ Set Up GitHub Secrets

1. Go to your GitHub repository
2. Navigate to **Settings** ‚Üí **Secrets and variables** ‚Üí **Actions**
3. Click **New repository secret**
4. Add these secrets:

| Secret Name | Value | Description |
|------------|-------|-------------|
| `YOUTUBE_API_KEY` | Your YouTube API key | Get from [Google Cloud Console](https://console.cloud.google.com/) |
| `YOUTUBE_CHANNEL_ID` | `UCUDKhNE_Y_DVkFx1nFNZTWQ` | Brahmakumaris channel ID |

### 3Ô∏è‚É£ Enable GitHub Actions

1. Go to **Actions** tab in your repository
2. If prompted, click **"I understand my workflows, go ahead and enable them"**
3. You should see the workflow **"YouTube Data Collection ETL"**

### 4Ô∏è‚É£ Test the Workflow

**Manual Test:**
1. Go to **Actions** tab
2. Click **YouTube Data Collection ETL**
3. Click **Run workflow** ‚Üí **Run workflow**
4. Watch it execute (takes ~2-5 minutes)
5. Check the `data/` directory for updated CSV

**Automatic Schedule:**
- Runs daily at **12:00 AM UTC** (5:30 AM IST)
- Automatically commits updated data to the repository

## üìä Connecting to Power BI

### Method 1: Direct GitHub URL (Recommended)

1. Open Power BI Desktop
2. **Get Data** ‚Üí **Web**
3. Use the raw GitHub URL:
   ```
   https://raw.githubusercontent.com/YOUR_USERNAME/YOUR_REPO/main/data/brahmakumaris_videos.csv
   ```
4. Click **OK** ‚Üí **Load**
5. Set up **Scheduled Refresh** in Power BI Service

### Method 2: Clone Repository Locally

1. Clone the repository:
   ```bash
   git clone https://github.com/YOUR_USERNAME/YOUR_REPO.git
   ```
2. In Power BI Desktop
   **Get Data** ‚Üí **Text/CSV**
3. Browse to `data/brahmakumaris_videos.csv`
4. Pull latest data with `git pull` before refreshing

### Method 3: Power BI Service (Premium)

For automatic refresh in Power BI Premium:

1. Publish your report to Power BI Service
2. Go to **Dataset settings**
3. **Scheduled refresh** ‚Üí **Add another time**
4. Set to refresh after GitHub Actions runs (e.g., 1:00 AM UTC)

## üîß Configuration

### Modify Schedule

Edit [`.github/workflows/youtube-etl.yml`](.github/workflows/youtube-etl.yml):

```yaml
on:
  schedule:
    - cron: '0 0 * * *'  # Daily at midnight UTC
    # Examples:
    # - cron: '0 */6 * * *'  # Every 6 hours
    # - cron: '0 0 * * 0'    # Weekly on Sunday
    # - cron: '0 12 * * *'   # Daily at noon UTC
```

### Modify Data Collection

Edit [`config.yaml`](config.yaml) to change:
- Days of history to collect
- Official podcast names
- Output fields

## üìà Monitoring

### View Run History
1. Go to **Actions** tab
2. Click on any workflow run
3. View logs, artifacts, and job summary

### Download Data Manually
1. Go to **Actions** tab ‚Üí Select a workflow run
2. Scroll to **Artifacts**
3. Download `youtube-data-XXX.zip`

### Check for Errors
- Workflow sends status in job summary
- Failed runs will show red X
- Click on failed run to see error logs

## üí∞ Cost

| Component | Cost |
|-----------|------|
| GitHub Actions | **Free** (2,000 min/month) |
| GitHub Storage | **Free** (1 GB) |
| YouTube API | **Free** (10,000 units/day) |
| **Total** | **$0/month** |

## üîí Security Notes

- ‚úÖ API keys stored in GitHub Secrets (encrypted)
- ‚úÖ Never commit API keys to repository
- ‚úÖ Workflow runs in isolated environment
- ‚úÖ Data commits use GitHub Actions bot account

## üêõ Troubleshooting

### Workflow not running
- Check if GitHub Actions is enabled
- Verify cron syntax
- Check repository permissions

### API errors
- Verify `YOUTUBE_API_KEY` secret is set correctly
- Check API quota in Google Cloud Console
- Ensure API key has YouTube Data API v3 enabled

### No data updates
- Check workflow run logs for errors
- Verify CSV file is being committed
- Ensure no file size limits exceeded

## üìö Next Steps

After this method is working:
- [ ] Set up Power BI dashboard
- [ ] Configure scheduled refresh
- [ ] Explore **n8n workflow** alternative
- [ ] Add more data sources (Instagram, Facebook)

## üîó Useful Links

- [GitHub Actions Documentation](https://docs.github.com/en/actions)
- [YouTube Data API](https://developers.google.com/youtube/v3)
- [Power BI Web Connector](https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-connect-to-web)
- [Cron Expression Generator](https://crontab.guru/)
